---
title: "Lab 2"
subtitle: "Penalized Regression"
author: "Shaina Trevino, Akhila Nekkanti, Johnathan Pedroza" 
date: "Assigned 4/21/20, Due 4/28/20"
output:
  html_document: 
    toc: true
    toc_float: true
    # theme: "journal"
    # css: "website-custom.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(here)
library(dplyr)
library(rio)

options(scipen = 999)

```

## Read in the `train.csv` data. Please feel free to use `sample_frac()` if you find that the data file is too large for your machine.

```{r, data}

d <- import(here("data", "train.csv"))
d <- d %>%  sample_frac(.2)

```


## 1. Initial Split

Set a seed and split the data into a training set and a testing set as two named objects. 

```{r, initial_split}
set.seed(3001)

d_split <- initial_split(d)

d_train <- training(d_split)
d_test  <- testing(d_split)

```

## 2. Resample

Set a seed and use 10-fold cross-validation to resample the traning data.

```{r, resample}
set.seed(3001)

cv_split <- vfold_cv(d_train, v = 10)

```

## 3. Preprocess

Complete the code maze below by filling in the blanks (____) to create a recipe object that includes:
* a formula model with `score` predicted by 4 predictors
* be sure there are no missing data in your predictors (try `step_naomit()`)
* center and scale all numeric predictors
* dummy code all nominal predictors

```{r, preprocess}

lasso4_rec <- 
  recipe(
    formula = score ~ enrl_grd + econ_dsvntg + sp_ed_fg + migrant_ed_fg, 
    data = d_train #use your training set here
  ) %>%
  step_naomit(everything(), skip = TRUE) %>% 
  step_string2factor(econ_dsvntg, sp_ed_fg, migrant_ed_fg) %>% 
  step_dummy(econ_dsvntg, sp_ed_fg, migrant_ed_fg) %>% 
  step_normalize(enrl_grd) 

```

## 4. Parsnip model

Create a `{parsnip}` lasso model where the penalty hyperparameter is set to be tuned.

```{r, lasso}

mod_lasso <- linear_reg() %>%
             set_engine("glmnet") %>%
             set_mode("regression") %>%
             set_args(penalty = tune(), # tune penalty
             mixture = 1) # specifies a lasso model

```

## 5. Fit a tuned lasso model

Complete the code maze below to fit a tuned lasso model.

```{r, lasso_fit_1}

lasso_grid <- grid_regular(penalty(), levels = 10)

lasso4_fit_1 <- tune_grid(
  mod_lasso,
  preprocessor = lasso4_rec,
  resamples = cv_split,
  grid = lasso_grid,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)

```

### Question A
  + How many models were fit to each fold of `lasso4_fit_1`? (Please provide a numeric answer, *and* use code to corroborate your answer.)
  
  + *There were 10 models fit for each fold*
  
  
```{r}
lasso4_fit_1 %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "rmse") %>% 
  count(id)
```

  + Use code to list the different values of `penalty()` that were used.

```{r}
lasso4_fit_1 %>% 
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  pull(penalty) #pulls a vector

```

## 6. Fit another tuned lasso model

Use your code from (5) above to complete the code maze below to fit a second tuned lasso model, using the same `parsnip` model, `recipe`, and resampled object you used before.

```{r, lasso_fit_2}

lasso4_fit_2 <- tune_grid(
  mod_lasso,
  preprocessor = lasso4_rec,
  resamples = cv_split,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)


```

### Question B

  + How many models were fit to each fold of `lasso4_fit_2`? (Please provide a numeric answer, *and* use code to corroborate your answer.)
  
  + *10 models for each fold were fit*

```{r}
lasso4_fit_2 %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "rmse") %>% 
  count(id)


```

  + If this is different than the number of models of `lasso4_fit_1`, please explain why.

+ *No they are the same. We set levels = 10 for the first lasso_fit and did not set levels for the second one so the default of 10 was used. Therefore, there are the same number of models that were fit for each fold.*

  + Use code to list the different values of `penalty()` that were used for *lasso4_fit_2*.

```{r}

p1 <- lasso4_fit_1 %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "rmse") %>% 
  pull(penalty)


p2 <- lasso4_fit_2 %>% 
  collect_metrics(summarize = FALSE) %>% 
  filter(.metric == "rmse") %>% 
  pull(penalty)

data.frame(p1, p2)

```

## 7. Complete the necessary steps to create and fit a tuned lasso model that has seven or more predictors (use any tuning grid you like). Note that you will need to create a new recipe as well.

```{r, lasso8}
#Recipe
lasso7_rec <- 
  recipe(
    formula = score ~ enrl_grd + econ_dsvntg + sp_ed_fg + migrant_ed_fg + gndr + ethnic_cd + trgt_assist_fg, 
    data = d_train #use your training set here
  ) %>%
  step_naomit(everything(), skip = TRUE) %>% 
  step_string2factor(econ_dsvntg, sp_ed_fg, migrant_ed_fg, gndr, ethnic_cd, trgt_assist_fg) %>% 
  step_dummy(econ_dsvntg, sp_ed_fg, migrant_ed_fg, gndr, ethnic_cd, trgt_assist_fg) %>% 
  step_normalize(enrl_grd) 

#Tuned grid
lasso7_fit_1 <- tune_grid(
  mod_lasso,
  preprocessor = lasso7_rec,
  resamples = cv_split,
  grid = 5,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)


```

## 8. Compare the metrics from the best lasso model with 4 predictors to the best lasso model with 7+ predicors. Which is best?

+ *Even with just 5 penalty values, the 7 predictor model was better than the 4 predictor model (see rmse below). Although the rmse is not vastly lower, so the model with less predictors could be preferred.*

```{r}
l4 <- lasso4_fit_2 %>% show_best(metric = "rmse", n = 1) %>% select(penalty, mean, .config)

l7 <- lasso7_fit_1 %>% show_best(metric = "rmse", n = 1) %>% select(penalty, mean, .config)

data.frame(l4, l7)

```

## 9. Fit a tuned elastic net model with the same predictors from (7). 
  + Create a new `{parsnip}` elastic net model
  + Use the same recipe from (7) above
  + Create and apply a regular grid for the elastic net model
  + Compare the metrics from the elastic net model to the best lasso model from (8). Which would you choose for your final model? What are the best hyperparameters for that model?
  
  + *The final model we would choose is the lasso model because they fit is very similar and the enet model is pretty much mimicing the lasso model. The lasso model has less penalties and is more interpretable.*

```{r enet}

mod_enet <- linear_reg() %>%
             set_engine("glmnet") %>%
             set_mode("regression") %>%
             set_args(penalty = tune(), # tune penalty
             mixture = tune()) # specifies a enet model

enet_params <- parameters(penalty(), mixture())
enet_grid <- grid_regular(enet_params, levels = c(10, 5))

#fit
enet_fit_1 <- tune_grid(
  mod_enet,
  preprocessor = lasso7_rec,
  resamples = cv_split,
  grid = enet_grid,
  control = tune::control_resamples(verbose = TRUE,
                                    save_pred = TRUE)
)

#compare
e1 <- enet_fit_1 %>% show_best(metric = "rmse", n = 1) %>% select(penalty, mixture, mean, .config)

data.frame(e1, l7)

```


